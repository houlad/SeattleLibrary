---
title: "Untitled"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(duckdb)
library(duckplyr)
source("helper_functions.R")

#Connect to local duckdb
con <- dbConnect(duckdb(), dbdir = "duckdb_monthly", read_only = FALSE)
con2 <- dbConnect(duckdb(), dbdir = "duckdb", read_only = FALSE)
```


```{r}
duckdb_read_csv(con, "checkouts_monthly", "/Users/athou/Downloads/Checkouts_by_Title_20250813.csv",
                col.types = c(
                  UsageClass = "VARCHAR",
                  CheckoutType = "VARCHAR",
                  MaterialType = "VARCHAR",
                  CheckoutYear = "BIGINT",
                  CheckoutMonth = "BIGINT",
                  Checkouts = "BIGINT",
                  Title = "VARCHAR",
                  ISBN = "VARCHAR",
                  Creator = "VARCHAR",
                  Subjects = "VARCHAR",
                  Publisher = "VARCHAR",
                  PublicationYear = "VARCHAR"
                ))
```

```{r}
checkouts_monthly <- tbl(con, 'checkouts_monthly')
checkouts_monthly 
```

```{r}
base_books <- checkouts_monthly |> 
  filter(UsageClass == "Physical", MaterialType == 'BOOK') |>
  select(-ISBN) |> 
  filter(!if_any(everything(), is.na)) 

base_books |> 
  group_by(CheckoutYear, CheckoutMonth) |> 
  summarise(avg_checkouts = mean(Checkouts)) |> 
  collect() |> 
  mutate(year_month = tsibble::make_yearmonth(year = CheckoutYear, month = CheckoutMonth)) |> 
  ggplot(aes(x = as.Date(year_month), y = avg_checkouts))+
  geom_line()  +
  geom_smooth()

base_books |> 
  group_by(Publisher) |> 
  summarise(total = n()) |> 
  arrange(desc(total)) |> 
  collect() |> 
  ungroup() |> 
  slice_head(n = 25) |> 
  ggplot(aes(x = total, y = fct_reorder(Publisher, total)))+
  geom_col()+
  scale_x_continuous(labels = scales::number_format())+
  theme_minimal()
```

## Building a test model for 10 books.
```{r}
base_books <- checkouts_monthly |> 
  filter(UsageClass == "Physical", MaterialType == 'BOOK') |>
  select(-ISBN) |> 
  filter(!if_any(everything(), is.na)) 

library(tidymodels)
base_books_df <- base_books |> 
  collect() |> 
  mutate(year_month = tsibble::make_yearmonth(year = CheckoutYear, month = CheckoutMonth),
         childrens_book = if_else(grepl("Children", Publisher), TRUE, FALSE)) 

#find the top 10 most checkouted out books across the whole dataset and then filter to just those
#books
top_10_books <- base_books_df |> 
  group_by(Title) |> 
  summarise(total_checks = sum(Checkouts)) |> 
  arrange(desc(total_checks)) |> 
  slice_head(n = 10) |> 
  pull(Title)

top_10_books_checkouts <- base_books_df |> 
  semi_join(data.frame(Title = top_10_books), join_by(Title)) |> 
  select(-UsageClass, -CheckoutType, -MaterialType, -Subjects) 

top_10_books_final <- top_10_books_checkouts |> 
  mutate(across(where(is.character), factor))

#using 90% of the data as training, 10% testing
books_split <- initial_time_split(top_10_books_final|> arrange(year_month), prop = .9)

training <- training(books_split)
testing <- testing(books_split)



xgboost_recipe <- recipe(
  Checkouts ~ CheckoutYear + CheckoutMonth + Title + Creator + Publisher + PublicationYear,
  data = training) |> 
  step_dummy(all_nominal_predictors()) 


xgboost_spec <- 
  boost_tree(trees = tune(),
             learn_rate = tune(),
             tree_depth = tune(),
             min_n = tune()) |> 
  set_mode("regression") |> 
  set_engine("xgboost")

xgboost_workflow <-
  workflow() |> 
  add_recipe(xgboost_recipe) |> 
  add_model(xgboost_spec)

xgb_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  learn_rate(),
  trees(),
  size = 10
)


folds <- vfold_cv(training, strata = Checkouts)
library(future)
library(tictoc)
tic()
plan(multisession, workers = 10)
xgboost_tune <- 
  tune_grid(xgboost_workflow, resamples = folds, grid = xgb_grid, 
            control = control_grid(verbose = TRUE))
toc()

best_fit <- select_best(xgboost_tune)

xgb_fit <- finalize_workflow(xgboost_workflow,
                             best_fit)

library(vip)
xgb_fit |> 
  fit(data = training) |> 
  extract_fit_parsnip() |> 
  vip(geom = 'point')

final <- last_fit(xgb_fit,
         books_split)

final |> 
  collect_predictions() 

final |> 
  collect_metrics()
```


#TODO Build modeling function; Build workflowset
Generating new variables to try to improve accuarcy
```{r}
base_books_df <- checkouts_monthly |>
  filter(UsageClass == "Physical", MaterialType == 'BOOK') |>
  select(-ISBN) |>
  filter(!if_any(everything(), is.na)) |>
  collect() |>
  mutate(year_month = tsibble::make_yearmonth(year = CheckoutYear, month = CheckoutMonth),
         childrens_book = if_else(grepl("Children", Publisher), TRUE, FALSE))


top_10_books <- base_books_df |>
  group_by(Title) |>
  summarise(total_checks = sum(Checkouts)) |>
  arrange(desc(total_checks)) |>
  slice_head(n = 10) |>
  pull(Title)

top_10_books_checkouts <- base_books_df |>
  semi_join(data.frame(Title = top_10_books), join_by(Title)) |>
  select(-UsageClass, -CheckoutType, -MaterialType, -Subjects)

top_10_books_final <- top_10_books_checkouts |>
  mutate(across(where(is.character), factor))

top_10_books_new_features_2 <- top_10_books_final |>
  group_by(Title, Publisher, PublicationYear) |>
  arrange(year_month) |>
  mutate(lag_1 = lag(Checkouts, 1),
         lag_2 = lag(Checkouts, 2),
         lag_6 = lag(Checkouts, 6),
         lag_12 = lag(Checkouts, 12),
         lag_18 = lag(Checkouts, 18),
         lag_24 = lag(Checkouts, 24),
         Checkout_book_sd = sd(Checkouts),
         Rolling_avg_2_month = zoo::rollmean(Checkouts, 2, fill = NA, align = 'right'),
         Rolling_avg_3_month = zoo::rollmean(Checkouts, 3, fill = NA, align = 'right'),
         Rolling_avg_6_month = zoo::rollmean(Checkouts, 6, fill = NA, align = 'right'),
         Rolling_avg_12_month = zoo::rollmean(Checkouts, 12, fill = NA, align = 'right'),
         is_new = as.factor(!duplicated(Title)),
         months_old = rank(year_month)) |>
  ungroup() |>
  group_by(year_month) |>
  mutate(avg_book_checkouts = mean(Checkouts),
         median_book_checkouts = median(Checkouts),
         month_sd = sd(Checkouts),
         is_summer = if_else(CheckoutMonth %in% c(6, 7, 8), 1, 0),
         is_december = if_else(CheckoutMonth == 12, 1, 0),
         is_childrens_book = if_else(childrens_book, 1, 0)) |>
  ungroup() |> 
  janitor::clean_names()


top_10_books_new_features <- checkouts_monthly |>
  select(-ISBN) |>
  build_base_dataset(condition = c(UsageClass == "Physical", MaterialType == 'BOOK',
                              !if_any(everything(), is.na)),
                variable = c(-UsageClass, -CheckoutType, -MaterialType,
                             -Subjects)) |> 
  select_top_n_books(.n = 10) |> 
  add_features(add_rolling_avg_vars = TRUE,
               add_lagged_vars = TRUE, 
               .n_lags = c(1, 2, 3, 6, 12),
               .n_avgs = c(2, 3, 4, 5, 6, 12 ))

```

##Model Specifications
```{r}
library(tidymodels)
library(future)
library(tictoc)
library(vip)
#using 90% of the data as training, 10% testing 
#This is ordered by year_month, so the last 10% of the dataset is used as testing set
#This ends up being Dec 2023 - July 2025
books_split <- initial_time_split(top_10_books_new_features|> arrange(year_month), prop = .9)
training <- training(books_split)
testing <- testing(books_split)

#xgboost_recipe
xgboost_recipe <- recipe(
  checkouts ~ checkout_year + checkout_month + publication_year + is_new +
              lag_1 + lag_2 + lag_3 + lag_6 + lag_12+ 
              rolling_avg_2_month + rolling_avg_3_month + rolling_avg_4_month + 
              rolling_avg_5_month + rolling_avg_6_month +
              avg_book_checkouts + is_summer + is_december +
              month_sd + months_old + median_book_checkouts+
              is_childrens_book+ checkout_book_sd,
  data = training) |> 
  step_dummy(all_nominal_predictors()) 

# specify spec
xgboost_spec <- 
  boost_tree(trees = tune(),
             learn_rate = tune(),
             tree_depth = tune(),
             min_n = tune()) |> 
  set_mode("regression") |> 
  set_engine("xgboost")

#build xgboost tuning grid
grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  learn_rate(),
  trees(),
  size = 10
)

# xgboost workflow
xgboost_workflow <-
  workflow() |> 
  add_recipe(xgboost_recipe) |> 
  add_model(xgboost_spec)

# generate folds for cross validation
folds <- vfold_cv(training)

#set of metrics to be used for tuning and evaluation
my_metrics <- metric_set(rmse, mape, mae)

#fit model and tune grid parameters
#using tic/toc to see how long this takes; using future::plan to fit in parallel
tic()
plan(multisession, workers = 14)
xgboost_tune <- 
  tune_grid(xgboost_workflow, resamples = folds, grid = grid, 
            control = control_grid(verbose = TRUE),#, save_pred = TRUE, save_workflow = TRUE),
            metrics = my_metrics)
toc()

#select the best tuning parameters
best_fit <- select_best(xgboost_tune)

#feed the optimal tuning parameters into the workflow
xgb_fit <- finalize_workflow(xgboost_workflow,
                             best_fit)

#with the best model in hand, fit the final model on the training set; then evaluate on the test set
final <- last_fit(xgb_fit,
         books_split)

#define which metrics I want to see
books_metrics <- metric_set(rmse, mape, mae)

#View those metrics
books_metrics(final |> collect_predictions(),
             truth = checkouts,
             estimate = .pred)


#bind predictions to our testing dataset and calculate residuals
#Just interested in looking at residuals really
final |> 
  collect_predictions() |> 
  zero_bound_predictions() |> 
  bind_cols(testing |>  select(-checkouts)) |> 
  mutate(resid = checkouts - .pred) |> View()


#View metrics with negative predictions bound to be equal to zero
books_metrics(
  final |> 
    collect_predictions() |>
    zero_bound_predictions(),
  truth = checkouts, estimate = .pred
  )

#Plot the most important features in the model
xgb_fit |> 
  fit(data = training) |> 
  extract_fit_parsnip() |> 
  vip(geom = 'point')

#plot predictions vs actuals
final |> 
  collect_predictions() |> 
  #zero_bound_predictions() |> 
  ggplot(aes(x = checkouts, y = .pred))+
  geom_point()+
  geom_abline(color = 'red')
```


```{r}
deployable_model <- xgb_fit |>  fit(training)
v <- vetiver_model(deployable_model, 'book_preds')
model_board <- pins::board_folder(path = 'data/testing', versioned = TRUE)
vetiver_pin_write(board = model_board, v)
vetiver_pin_read(model_board, book_preds)

plumber::pr() |> 
  vetiver::vetiver_api(v) |> 
  plumber::pr_run()

base_url <- "127.0.0.1:24525/"
url <- paste0(base_url, 'ping')
r = httr::GET(url)
```

##Lightgbm Fit
```{r}
library(tidymodels)
library(bonsai)
top_10_books_new_features <- top_10_books_final |> 
  group_by(Title, Publisher, PublicationYear) |> 
  arrange(year_month) |> 
  mutate(Checkout_lag_1_month = lag(Checkouts, 1),
         Checkout_lag_2_month = lag(Checkouts, 2),
         Checkout_lag_6_month = lag(Checkouts, 6),
         Rolling_average_2_month = zoo::rollmean(Checkouts, 2, fill = NA),
         Rolling_average_3_month = zoo::rollmean(Checkouts, 3, fill = NA),
         Rolling_average_6_month = zoo::rollmean(Checkouts, 6, fill = NA),
         Rolling_average_12_month = zoo::rollmean(Checkouts, 12, fill = NA)) |> 
  ungroup() |> 
  group_by(year_month) |> 
  mutate(average_book_checkouts = mean(Checkouts),
         is_summer = if_else(CheckoutMonth %in% c(6, 7, 8), 1, 0),
         is_december = if_else(CheckoutMonth == 12, 1, 0)) |> 
  ungroup()

#using 90% of the data as training, 10% testing
books_split <- initial_time_split(top_10_books_new_features|> arrange(year_month), prop = .9)

training <- training(books_split)
testing <- testing(books_split)



lightgbm_recipe <- recipe(
  Checkouts ~ CheckoutYear + CheckoutMonth + PublicationYear +
              Checkout_lag_1_month + Checkout_lag_2_month + Checkout_lag_6_month +
              Rolling_average_2_month + Rolling_average_6_month + Rolling_average_12_month+
              average_book_checkouts + is_summer + is_december + Rolling_average_3_month,
  data = training) |> 
  step_dummy(all_nominal_predictors()) 


lightgbm_spec <- 
  boost_tree(trees = tune(),
             learn_rate = tune(),
             tree_depth = tune(),
             min_n = tune()) |> 
  set_mode("regression") |> 
  set_engine("lightgbm")

lightgbm_workflow <-
  workflow() |> 
  add_recipe(lightgbm_recipe) |> 
  add_model(lightgbm_spec)

lightgbm_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  learn_rate(),
  trees(),
  size = 10
)

my_metrics <- metric_set(rmse, mape, mae)
folds <- vfold_cv(training, strata = Checkouts)
library(future)
library(tictoc)
tic()
plan(multisession, workers = 14)
lightgbm_tune <- 
  tune_grid(lightgbm_workflow, resamples = folds, grid = lightgbm_grid, 
            control = control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE),
            metrics = my_metrics)
toc()

best_fit_lightgbm <- select_best(lightgbm_tune)

lightgbm_fit <- finalize_workflow(lightgbm_workflow,
                             best_fit_lightgbm)

library(vip)
lightgbm_fit |> 
  fit(data = training) |> 
  extract_fit_parsnip() |> 
  vip(geom = 'point')

final_lightgbm <- last_fit(lightgbm_fit,
         books_split)

final_lightgbm |> 
  collect_predictions() |> View()

final_lightgbm |> 
  collect_metrics()
```

## Ensembling XGBoost and LightGBM
```{r}
library(bonsai)
library(stacks)
books_split <- initial_time_split(top_10_books_new_features|> arrange(year_month), prop = .9)

training <- training(books_split)
testing <- testing(books_split)

folds <- vfold_cv(training, strata = Checkouts)

xgboost_recipe <- recipe(
  Checkouts ~ CheckoutYear + CheckoutMonth + Title + Publisher + Creator + PublicationYear +
              Checkout_lag_1_month + Checkout_lag_2_month + Checkout_lag_6_month +
              Rolling_average_2_month + Rolling_average_6_month + Rolling_average_12_month+
              average_book_checkouts + is_summer + is_december + Rolling_average_3_month,
  data = training) |> 
  step_dummy(all_nominal_predictors()) 

xgboost_spec <- 
  boost_tree(trees = tune(),
             learn_rate = tune(),
             tree_depth = tune(),
             min_n = tune()) |> 
  set_mode("regression") |> 
  set_engine("xgboost")

lightgbm_recipe <- recipe(
  Checkouts ~ CheckoutYear + CheckoutMonth + PublicationYear +
              Checkout_lag_1_month + Checkout_lag_2_month + Checkout_lag_6_month +
              Rolling_average_2_month + Rolling_average_6_month + Rolling_average_12_month+
              average_book_checkouts + is_summer + is_december + Rolling_average_3_month,
  data = training) |> 
  step_dummy(all_nominal_predictors()) 


lightgbm_spec <- 
  boost_tree(trees = tune(),
             learn_rate = tune(),
             tree_depth = tune(),
             min_n = tune()) |> 
  set_mode("regression") |> 
  set_engine("lightgbm")

all_workflows <- 
  workflow_set(
    preproc = list(xgboost = xgboost_recipe,
                   lightgbm = lightgbm_recipe),
    models = list(XGBOOST = xgboost_spec,
                  LIGHTGBM = lightgbm_spec)
  )



lightgbm_grid <- grid_space_filling(
  tree_depth(),
  min_n(),
  learn_rate(),
  trees(),
  size = 10
)

grid_results <- all_workflows |> 
  workflow_map(
    resamples = folds,
    grid = lightgbm_grid, 
    control = control_grid(verbose = TRUE, save_pred = TRUE, save_workflow = TRUE,
                           parallel_over = "everything")
  )

library(stacks)
books_stack <-
  stacks() |> 
  add_candidates(grid_results)

ens <- blend_predictions(books_stack, penalty = 10^seq(-2, -0.5, length = 20))

autoplot(ens)

ens <- fit_members(ens)

ens_test_pred <- 
  predict(ens, testing) |> 
  bind_cols(testing)

ens_test_pred |> 
  books_metrics(Checkouts, .pred)
```



##ARIMA and ETS fit on the same dataset
```{r}
library(tsibble)
top_10_books_ts_df <- top_10_books_checkouts |> 
  mutate(across(where(is.character), factor)) |>
  as_tsibble(key = c(Title, Publisher, PublicationYear), index = year_month) |> 
  fill_gaps(Checkouts = 0)

top_10_books_ts_df |> 
  ggplot(aes(x = year_month, y = Checkouts, color = Title))+
  geom_line()


top_10_books_ts_training <- top_10_books_ts_df |>  filter(year_month < yearmonth('2023 Nov'))
top_10_books_ts_testing <- top_10_books_ts_df |>  filter(year_month >= yearmonth('2023 Nov'))

library(fable)
tic()
plan(multisession, workers = 10)
top_10_books_ts_training |> 
  model(
    ets = ETS(Checkouts),
    arima = ARIMA(Checkouts),
    snaive = SNAIVE(Checkouts)
  )
toc()

test_forecast <- top_10_books_ts_training |> 
  model(
    ets = ETS(Checkouts),
    arima = ARIMA(Checkouts),
    snaive = SNAIVE(Checkouts)
  ) 
test_forecast |> 
  forecast(h = '20 months') |> 
  filter(Title == 'Educated : a memoir / Tara Westover.') |> View()
  autoplot(filter(top_10_books_ts_training, Title == 'Educated : a memoir / Tara Westover.'), level =NULL)
```

